#!/usr/bin/env python3
"""Mock squeue command for testing."""

import argparse
import random

# Constants for parsing
# All-users tuples now include Partition:
# (JobID, Name, User, Partition, State, Time, Nodes, NodeList, [TRES])
MIN_JOB_FIELDS_WITH_USER = 8
MIN_JOB_FIELDS_WITH_TRES = 9
MIN_JOB_FIELDS_WITHOUT_USER = 8
MIN_USER_INDEX = 2
MIN_ALL_USERS_STATE_INDEX = 4
MIN_USER_ONLY_STATE_INDEX = 2
MIN_RANDOM_JOBS = 2
MAX_RANDOM_JOBS = 10
LEGACY_TUPLE_LEN_7 = 7
LEGACY_TUPLE_LEN_8 = 8

# Real job data from current cluster (with user field and TRES)
MOCK_JOBS_ALL_USERS = [
    (
        "47441",
        "rope_pct_llama_",
        "afkhan",
        "gpu",
        "PENDING",
        "0:00",
        "8",
        "(Priority)",
        "cpu=64,mem=512G,node=8,gres/gpu=32",
    ),
    (
        "47434",
        "rope_pct_llama_",
        "afkhan",
        "gpu",
        "PENDING",
        "0:00",
        "8",
        "(Resources)",
        "cpu=64,mem=512G,node=8,gres/gpu=32",
    ),
    # Pending array job with 50 tasks - demonstrates array expansion
    (
        "47700_[0-49]",
        "array_train",
        "testuser",
        "gpu",
        "PENDING",
        "0:00",
        "1",
        "(Priority)",
        "cpu=8,mem=32G,gres/gpu=1",
    ),
    # Pending array job with throttle - 100 tasks with %10 throttle
    (
        "47701_[0-99%10]",
        "throttled_array",
        "testuser",
        "gpu",
        "PENDING",
        "0:00",
        "1",
        "(Resources)",
        "cpu=4,mem=16G,gres/gpu:h200=2",
    ),
    (
        "47587",
        "sam3_multigpu",
        "flkar",
        "gpu",
        "RUNNING",
        "5:29:36",
        "1",
        "daisg106",
        "cpu=192,mem=2000G,node=1,gres/gpu=8",
    ),
    (
        "47474_5",
        "sdvae_sit_b_2_r",
        "xichen",
        "gpu",
        "RUNNING",
        "18:29",
        "1",
        "daisg114",
        "cpu=192,mem=2000G,node=1,gres/gpu=8",
    ),
    (
        "47475_2",
        "sdvae_sit_b_2_r",
        "xichen",
        "gpu",
        "RUNNING",
        "32:59",
        "1",
        "daisg104",
        "cpu=192,mem=2000G,node=1,gres/gpu=8",
    ),
    (
        "47429",
        "train",
        "rmaser",
        "gpu",
        "RUNNING",
        "12:30:07",
        "1",
        "daisg116",
        "cpu=192,mem=2000G,node=1,gres/gpu=8",
    ),
    (
        "46043",
        "foldmae",
        "dchen",
        "gpu",
        "RUNNING",
        "12:58:54",
        "4",
        "daisg[111-113,115]",
        "cpu=768,mem=8000G,node=4,gres/gpu=32",
    ),
    (
        "47537",
        "foldmae",
        "dchen",
        "gpu",
        "PENDING",
        "0:00",
        "4",
        "(Priority)",
        "cpu=768,mem=8000G,node=4,gres/gpu=32",
    ),
    (
        "47670",
        "17_imagenet128_",
        "bpogodzi",
        "gpu",
        "PENDING",
        "0:00",
        "1",
        "(Nodes requi",
        "cpu=192,mem=2000G,node=1,gres/gpu=8",
    ),
    (
        "47671",
        "17_imagenet128_",
        "bpogodzi",
        "gpu",
        "PENDING",
        "0:00",
        "1",
        "(Priority)",
        "cpu=192,mem=2000G,node=1,gres/gpu=8",
    ),
    (
        "47462_11",
        "decomposed_rep_",
        "jvanders",
        "gpu",
        "RUNNING",
        "4:28",
        "1",
        "daisg103",
        "cpu=192,mem=2000G,node=1,gres/gpu=8",
    ),
    (
        "47458_11",
        "decomposed_rep_",
        "jvanders",
        "RUNNING",
        "8:59",
        "1",
        "daisg102",
        "cpu=192,mem=2000G,node=1,gres/gpu=8",
    ),
    (
        "47461_11",
        "lora_rep_rope_f",
        "jvanders",
        "RUNNING",
        "8:59",
        "1",
        "daisg103",
        "cpu=192,mem=2000G,node=1,gres/gpu=8",
    ),
    (
        "47457_11",
        "lora_rep_rope_r",
        "jvanders",
        "RUNNING",
        "12:29",
        "1",
        "daisg101",
        "cpu=192,mem=2000G,node=1,gres/gpu=8",
    ),
    ("47466_2", "train", "rmaser", "RUNNING", "10:10:41", "1", "daisg108", "cpu=192,mem=2000G,node=1,gres/gpu=8"),
    ("47466_3", "train", "rmaser", "RUNNING", "10:10:41", "1", "daisg109", "cpu=192,mem=2000G,node=1,gres/gpu=8"),
    ("47466_4", "train", "rmaser", "RUNNING", "10:10:41", "1", "daisg110", "cpu=192,mem=2000G,node=1,gres/gpu=8"),
    (
        "47649_8",
        "pat_commonsense",
        "adhikar",
        "RUNNING",
        "2:18:57",
        "1",
        "daisg105",
        "cpu=192,mem=2000G,node=1,gres/gpu=8",
    ),
    (
        "47649_6",
        "pat_commonsense",
        "adhikar",
        "RUNNING",
        "2:28:06",
        "1",
        "daisg107",
        "cpu=192,mem=2000G,node=1,gres/gpu=8",
    ),
    (
        "47649_0",
        "pat_commonsense",
        "adhikar",
        "RUNNING",
        "2:28:32",
        "1",
        "daisg110",
        "cpu=192,mem=2000G,node=1,gres/gpu=8",
    ),
]

# Jobs without user field (for -u flag)
# Tuple format: (JobID, Name, State, Time, Nodes, NodeList, SubmitTime, StartTime)
MOCK_JOBS_USER_ONLY = [
    (
        "12345",
        "train_model",
        "RUNNING",
        "2:34:56",
        "4",
        "gpu-node-[01-04]",
        "2024-01-15T10:30:00",
        "2024-01-15T10:35:00",
    ),
    (
        "12346",
        "preprocess",
        "RUNNING",
        "0:45:12",
        "1",
        "cpu-node-05",
        "2024-01-15T11:00:00",
        "2024-01-15T11:05:00",
    ),
    (
        "12347",
        "evaluate",
        "PENDING",
        "0:00",
        "2",
        "(Priority)",
        "2024-01-15T11:30:00",
        "Unknown",
    ),
    (
        "12348",
        "inference",
        "PENDING",
        "0:00",
        "1",
        "(Resources)",
        "2024-01-15T11:45:00",
        "Unknown",
    ),
    (
        "12349",
        "data_load",
        "RUNNING",
        "1:23:45",
        "1",
        "gpu-node-08",
        "2024-01-15T09:00:00",
        "2024-01-15T09:05:00",
    ),
]


def _normalize_all_users_job(job: tuple[str, ...]) -> tuple[str, ...]:
    """Normalize all-users job tuples to include Partition.

    Historical versions of this mock stored tuples as:
      (JobID, Name, User, State, Time, Nodes, NodeList, [TRES])

    The production app now expects:
      (JobID, Name, User, Partition, State, Time, Nodes, NodeList, [TRES])
    """
    if len(job) == LEGACY_TUPLE_LEN_7:
        job_id, name, user, state, time, nodes, nodelist = job
        return (job_id, name, user, "gpu", state, time, nodes, nodelist)
    if len(job) == LEGACY_TUPLE_LEN_8:
        job_id, name, user, state, time, nodes, nodelist, tres = job
        return (job_id, name, user, "gpu", state, time, nodes, nodelist, tres)
    return job


def print_fixed_width(jobs: list[tuple[str, ...]]) -> None:
    """Print jobs in fixed width format compatible with -O."""
    # JobID:20,Name:20,UserName:15,Partition:15,StateCompact:10,TimeUsed:12,NumNodes:6,NodeList:30,tres:80
    for raw_job in jobs:
        job = _normalize_all_users_job(raw_job)
        if len(job) < MIN_JOB_FIELDS_WITH_USER:
            continue

        job_id, name, user, partition, state, time, nodes, nodelist = job[:MIN_JOB_FIELDS_WITH_USER]
        tres = job[8] if len(job) >= MIN_JOB_FIELDS_WITH_TRES else ""

        # Pad fields to match expected fixed width
        line = f"{job_id:<20}{name:<20}{user:<15}{partition:<15}{state:<10}{time:<12}{nodes:<6}{nodelist:<30}{tres}"
        print(line)


def _filter_by_state(
    jobs: list[tuple[str, ...]],
    allowed_states: list[str],
    state_index: int,
) -> list[tuple[str, ...]]:
    """Filter jobs by allowed states."""
    return [j for j in jobs if len(j) > state_index and any(s in j[state_index] for s in allowed_states)]


def main() -> None:
    """Main entry point for mock squeue command."""
    parser = argparse.ArgumentParser()
    parser.add_argument("-u", "--user", default=None)
    parser.add_argument("-o", "--format", default=None)
    parser.add_argument("-O", "--Format", default=None)
    parser.add_argument("-a", "--all", action="store_true")
    parser.add_argument("-t", "--states", default=None) # Add support for -t
    parser.add_argument("--noheader", action="store_true") # Add support for --noheader

    args = parser.parse_args()

    # Determine mode
    is_fixed_width = args.Format is not None

    # Filter states if specified
    allowed_states = args.states.split(",") if args.states else None

    # Get job pool
    using_all_users_jobs = False
    jobs: list[tuple[str, ...]]
    if is_fixed_width:
        # Fixed width mode implies full job details including TRES
        jobs = [_normalize_all_users_job(j) for j in MOCK_JOBS_ALL_USERS]
        using_all_users_jobs = True
    else:
        # Legacy/Simple mode
        # Determine format - check if user field is in format string
        has_user_field = args.format and "%.8u" in args.format
        is_all_users = args.all or (not args.user and has_user_field)

        if is_all_users and has_user_field:
            jobs = [_normalize_all_users_job(j) for j in MOCK_JOBS_ALL_USERS]
            using_all_users_jobs = True
        else:
            jobs = list(MOCK_JOBS_USER_ONLY)

    # Filter by state
    if allowed_states:
        state_idx = MIN_ALL_USERS_STATE_INDEX if using_all_users_jobs else MIN_USER_ONLY_STATE_INDEX
        jobs = _filter_by_state(jobs, allowed_states, state_idx)

    # Filter by user if specified
    if args.user and using_all_users_jobs:
        jobs = [job for job in jobs if len(job) > MIN_USER_INDEX and job[MIN_USER_INDEX] == args.user]

    # Randomly select some jobs to simulate dynamic queue
    # Ensure at least some jobs if available
    if not jobs:
        selected_jobs: list[tuple[str, ...]] = []
    else:
        num_jobs = random.randint(  # noqa: S311
            min(len(jobs), MIN_RANDOM_JOBS),
            min(len(jobs), MAX_RANDOM_JOBS),
        )
        selected_jobs = random.sample(jobs, num_jobs)

    if is_fixed_width:
        print_fixed_width(selected_jobs)
        sys.exit(0)

    # Regular output formatting
    has_user_field = args.format and "%.8u" in args.format
    has_tres_field = args.format and "%t" in args.format

    if not args.noheader:
        if using_all_users_jobs:
             if has_tres_field:
                print("     JOBID|           NAME|    USER|   STATE|      TIME|NODE|NODELIST(REASON)|TRES")
             else:
                print("     JOBID|           NAME|    USER|   STATE|      TIME|NODE|NODELIST(REASON)")
        else:
            print("     JOBID|           NAME|    USER|   STATE|      TIME|NODE|NODELIST(REASON)")
    else:
        print("JOBID|JOBNAME|STATE|TIME|NODES|NODELIST|SUBMIT_TIME|START_TIME")


def _print_jobs(jobs: list[tuple[str, ...]], using_all_users: bool, has_tres: bool) -> None:
    """Print the job rows."""
    for job in jobs:
        if using_all_users:
            job_id, name, user, _partition, state, time, nodes, nodelist = job[:MIN_JOB_FIELDS_WITH_USER]
            if has_tres and len(job) >= MIN_JOB_FIELDS_WITH_TRES:
                tres = job[8]
                print(f"{job_id:>10}|{name:>15}|{user:>8}|{state:>8}|{time:>10}|{nodes:>4}|{nodelist:>12}|{tres}")
            else:
                print(f"{job_id:>10}|{name:>15}|{user:>8}|{state:>8}|{time:>10}|{nodes:>4}|{nodelist:>12}")
        else:
            job_id, name, state, time, nodes, nodelist, submit_time, start_time = job[:MIN_JOB_FIELDS_WITHOUT_USER]
            print(f"{job_id}|{name}|{state}|{time}|{nodes}|{nodelist}|{submit_time}|{start_time}")


if __name__ == "__main__":
    main()
